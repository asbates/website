---
title: What is Bayesian Statistics?
author: Andrew Bates
date: '2018-03-29'
slug: what-is-bayesian-statistics
categories:
  - Bayesian Statistics
tags: []
---



<p>As of now I am in the middle of a Bayesian statistics course at San Diego State University. The course is interesting but one thing was particularly troubling when the course first started: I didn’t know what made Bayesian statistics different from all the classical (or frequentist) courses I had taken previously. Hopefully this post will help clarify things for others who are just starting to learn about Bayesian statistics.</p>
<p>One of the reasons why it took me a while (a couple weeks) to understand the difference between Bayesian and frequentists statistics is that I could not find a satisfactory explanation online. I did find a number of posts, lecture notes, etc. that tried to explain the difference but they didn’t really help that much. Most of them either said something along the lines of “the difference is in the interpretation of probability”, or gave some sort of explanation through a specific example without any math/stats (which is almost never helpful for me), or just explained Bayes rule in the discrete case (which is not exclusive to Bayesian statistics). These approaches are not necessarily wrong, I just felt they were too simplified for someone like me who has a math and stats background.</p>
<div id="they-bayesian-approach" class="section level3">
<h3>They Bayesian Approach</h3>
<p>To remedy this, I would like to give my idea of the difference between Bayesian and frequentist statistics. For me, the difference is easiest to understand with the following table:</p>
<table>
<thead>
<tr class="header">
<th align="center"></th>
<th align="center">Bayesian</th>
<th align="center">Frequentist</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">parameter</td>
<td align="center">random</td>
<td align="center">fixed</td>
</tr>
<tr class="even">
<td align="center">data</td>
<td align="center">fixed</td>
<td align="center">random</td>
</tr>
</tbody>
</table>
<p>To elaborate, given a model for the data, a frequentist views the parameters of that model to be some fixed (but unknown) value and the data to come from a random variable. The view is that the data will be generated by some model with a given value for the parameters. The randomness comes from the data itself as each sample we take will be different.</p>
<p>A Bayesian takes sort of the opposite approach. Like a frequentist, the data is viewed as coming from a model with a set of parameters. But the difference is that the parameters of the model are considered random and the data is viewed as fixed once it is observed.</p>
<p>To me, this is the fundamental difference between the classical and Bayesian approaches: how parameters and data are treated. The common explanations I mentioned above don’t capture this. One is more of a philosophical difference and the others, although they may be useful for some, are too simplified and I don’t think they really add anything to what a first course in probability or statistics would cover.</p>
<p>This difference imparts an approach to Bayesian and frequentist inference. In the frequentist world, we want to estimate the parameter (a fixed value) along with a confidence interval and maybe a p-value. We then use that estimate to infer the model for the population. In the Bayesian context, we want to find the (new) distribution of the parameter, given the data. This lets us infer how our parameter has changed based on the data and gives us a current, updated estimate.</p>
</div>
<div id="an-example" class="section level3">
<h3>An Example</h3>
<p>To illustrate the Bayesian approach, let’s consider a simple example: Binomial data. Suppose a single data point that is Binomial with parameters <span class="math inline">\(n\)</span> and <span class="math inline">\(p\)</span> where <span class="math inline">\(n\)</span> is known (this is the same as <span class="math inline">\(n\)</span> Bernoulli data points). In the Bayesian approach, we consider <span class="math inline">\(p\)</span> to be a random variable. To make the math easier, we consider <span class="math inline">\(p\)</span> to follow a Beta distribution. Note that this is not necessary, <span class="math inline">\(p\)</span> could have whatever distribution we want as long as it meets the conditions for a Binomial (between 0 and 1). We write this model as follows:</p>
<p><span class="math display">\[X| p \sim \mathrm{Binomial}(n,p) \]</span> where <span class="math display">\[ p \sim \mathrm{Beta}(a,b). \]</span></p>
<p>The first line is the model for the data and the second is the model for the <span class="math inline">\(p\)</span>. The distribution for <span class="math inline">\(p\)</span> is called the <em>prior distribution</em> (remember that we know <span class="math inline">\(n\)</span> here). The parameters <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> for <span class="math inline">\(p\)</span> are chosen independently of viewing the data based on an expert opinion (we won’t get into how to do that here). What we are interested in is finding the <em>posterior distribution</em> of <span class="math inline">\(p\)</span> given the data. That is, we want to find <span class="math inline">\(f(p|X_1, \ldots, X_n)\)</span>. This will give us insight into how <span class="math inline">\(p\)</span> behaves and can be used as the prior distribution for a future data analysis.</p>
<p>Because we chose the distribution for <span class="math inline">\(p\)</span> to be a beta, its not too difficult to find the posterior. We just need to invoke Bayes rule (this is a generalization of Bayes rule in the discrete case):</p>
<p><span class="math display">\[ f(p|X) = \frac{f(X|p)f(p)}{\int_0^1f(X|p)f(p) \hspace{3pt} dp}. \]</span></p>
<p>It turns out we can simplify the above expression because the denominator is just a normalizing constant to ensure the density integrates to 1. We can also simplify by ignoring the constants for each of the densities. We only need to figure out what is called the kernel of the density because this is enough to tell us the distribution. So we can find the posterior as follows:</p>
<span class="math display">\[\begin{align*}
f(p|X) &amp;\propto p^x(1-p)^{n-x}  p^{a-1} (1-p)^{b-1} \\
&amp;= p^{(x+a)-1}(1-p)^{(n-x+b)-1}.
\end{align*}\]</span>
<p>This looks like a Beta distribution with parameters <span class="math inline">\(x+a\)</span> and <span class="math inline">\(n-x+b\)</span>. We could have worked this out by putting in all the distribution constants and evaluating the integral but its a lot easier not to and the math can be done to show why that is o.k.</p>
</div>
<div id="summary" class="section level3">
<h3>Summary</h3>
<p>To reiterate, I think the difference between the Bayesian and frequentist approach to statistics boils down to how the data and parameters of a model are treated (fixed vs. random). I also think it’s easier to understand the difference in this way particularly for someone who already knows a bit about statistics. Hopefully this post will help to clarify things for those who end up in a similar situation as I was in.</p>
</div>
