---
title: What is Bayesian Statistics?
author: Andrew Bates
date: '2018-03-29'
slug: what-is-bayesian-statistics
categories:
  - Bayesian Statistics
tags: []
---



<p>I’m currently in the middle of a Bayesian statistics course. It’s exciting for several reasons (yes, I get excited about statistics courses) but one thing was particularly troubling when it first started: I didn’t fully understand what made Bayesian statistics different from all the classical (or frequentist) courses I had taken previously. Hopefully this post will help clarify things for others who are just starting to learn about Bayesian statistics.</p>
<p>One of the reasons why it took me a bit to understand the fundamental difference between Bayesian and frequentist statistics is that I could not find a satisfactory explanation online. I did find a number of posts, lecture notes, etc. that tried to explain the difference but they didn’t really help that much. Most of them either said something along the lines of “the difference is in the interpretation of probability”, or gave an explanation through a specific example (which is rarely helpful for me until I’ve seen some math), or just explained Bayes rule which is not exclusive to Bayesian statistics. These approaches are not necessarily wrong, I just felt they were either too simplified for someone like me who has a math and stats background or they didn’t really get at the core of the difference in the two approaches.</p>
<div id="they-bayesian-approach" class="section level3">
<h3>They Bayesian Approach</h3>
<p>To remedy this, I would like to give my take on the difference between Bayesian and frequentist statistics. For me, the difference is easiest to understand with the following table:</p>
<table>
<thead>
<tr class="header">
<th align="center"></th>
<th align="center">Bayesian</th>
<th align="center">Frequentist</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center"><strong>parameter</strong></td>
<td align="center">random</td>
<td align="center">fixed</td>
</tr>
<tr class="even">
<td align="center"><strong>data</strong></td>
<td align="center">fixed</td>
<td align="center">random</td>
</tr>
</tbody>
</table>
<p>To elaborate, given a model for the data, a frequentist<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a> views the parameters of that model to be some fixed (but unknown) value and the data to be random. The view is that the data will be generated by some “true” model with a given value for the parameters. The randomness comes from the data itself as each sample we take will be different.</p>
<p>A Bayesian takes sort of the opposite approach. Like a frequentist, the data is viewed as coming from a model with a set of parameters. But the parameters of the model are considered random and the data is viewed as fixed.</p>
<p>To me, this is the fundamental difference between the classical and Bayesian approaches: how parameters and data are treated. The common explanations I mentioned above don’t capture this. One is more of a philosophical difference and the others, although they may be useful for some, are too simplified and I don’t think they really add anything to what a first course in probability or statistics would cover.</p>
<p>This difference in how data and parameters are treated imparts a difference in how inference is approached. In the frequentist world, we want to estimate the parameter (a fixed value) along with a confidence interval and typically a p-value. We then use that estimate to infer the model for the entire population. In the Bayesian context, we want to find the (new) distribution of the parameter, given the data. This lets us infer how the distribution of our parameter has changed based on the data at hand and gives us a current, updated estimate of the distribution of that parameter.</p>
</div>
<div id="an-example" class="section level3">
<h3>An Example</h3>
<p>To illustrate the Bayesian approach, let’s consider a simple example. Suppose we have data from a Binomial distribution with parameters <span class="math inline">\(n\)</span> and <span class="math inline">\(p\)</span> where <span class="math inline">\(n\)</span> is known. In the Bayesian approach, we consider <span class="math inline">\(p\)</span>, the success probability, to be a random variable instead of a fixed value. Here, we consider <span class="math inline">\(p\)</span> to follow a Beta distribution. This distributional assumption is not necessary. <span class="math inline">\(p\)</span> could have whatever distribution we want as long as it meets the conditions for a Binomial (between 0 and 1). However, it makes the math easy and the Beta distribution is flexible enough that it should suffice in most cases. We can write this model as follows:</p>
<p><span class="math display">\[X| p \sim \mathrm{Binomial}(n,p) \]</span> where <span class="math display">\[ p \sim \mathrm{Beta}(a,b). \]</span></p>
<p>The first equation is the model for the data (a conditional distribution) and the second is the model for the success probability <span class="math inline">\(p\)</span>. The distribution for <span class="math inline">\(p\)</span> is called the <strong>prior distribution</strong>. The reason for this is that we think of having some knowledge about <span class="math inline">\(p\)</span> before data is collected. For example, we think <span class="math inline">\(p\)</span> can be no larger than 0.8. The parameters <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> for <span class="math inline">\(p\)</span> are called <strong>hyperparameters</strong>. They are set independently of the data based on our prior knowledge. What we are interested in is finding the <strong>posterior distribution</strong> of <span class="math inline">\(p\)</span> given the data. That is, we want to find <span class="math inline">\(f(p|X)\)</span>. This will give us insight into how <span class="math inline">\(p\)</span> behaves and can be used as the prior distribution for a future data analysis.</p>
<p>Because we chose the distribution for <span class="math inline">\(p\)</span> to be a Beta, it’s not too difficult to find the posterior. We just need to invoke Bayes rule:</p>
<p><span class="math display">\[ f(p|X) = \frac{f(X|p)f(p)}{\int_0^1f(X|p)f(p) \hspace{3pt} dp}. \]</span></p>
Substituting the appropriate densities gives
<span class="math display">\[\begin{align*}
f(p|X) &amp;= \dfrac{\binom{n}{x}p^x(1-p)^{n-x} \frac{\Gamma(a+b)}{\Gamma(a)\Gamma(b)}p^{a-1}(1-p)^{b-1}}{\int_0^1 \binom{n}{x}p^x(1-p)^{n-x} \frac{\Gamma(a+b)}{\Gamma(a)\Gamma(b)} p^{a-1}(1-p)^{b-1} \hspace{3pt} dp} \\
&amp;= \dfrac{p^{(x+a)-1}(1-p)^{(n-x+b)-1}}{\int_0^1 p^{(x+a)-1}(1-p)^{(n-x+b)-1} \hspace{3pt} dp} \\
&amp;= \dfrac{p^{(x+a)-1}(1-p)^{(n-x+b)-1}}{\frac{\Gamma(x+a)\Gamma(n-x+b)}{\Gamma(n+a+b)}}
\end{align*}\]</span>
<p>This may look ugly and complicated at first glance so let’s walk through it. The first line is simply substituting <span class="math inline">\(f(X|p)\)</span> and <span class="math inline">\(f(p)\)</span> based on the Binomial and Beta distributions. Next, all the constants to make them densities can cancel. For the last line, note that the numerator is the “meat” of a Beta<span class="math inline">\((x+a, n-x+b)\)</span> distribution. So the integral in the denominator is just the inverse of the constant for the Beta distribution.</p>
<p>It turns out we can simplify the above expression because the denominator is just a normalizing constant to ensure the density integrates to 1. We can also simplify by ignoring the constants for each of the densities because they will cancel out since the integration is with respect to <span class="math inline">\(p\)</span>. We only need to figure out the kernel of the density because this is enough to tell us the distribution. This is illustrated as follows:</p>
<span class="math display">\[\begin{align*}
f(p|X) &amp;\propto p^x(1-p)^{n-x}  p^{a-1} (1-p)^{b-1} \\
&amp;= p^{(x+a)-1}(1-p)^{(n-x+b)-1}.
\end{align*}\]</span>
<p>This looks like a Beta distribution with parameters <span class="math inline">\(x+a\)</span> and <span class="math inline">\(n-x+b\)</span>. Typically, this is how posteriors are worked out because it lets us focus on the important bits without getting bogged down by writing extra stuff that we know will cancel anyways.</p>
</div>
<div id="summary" class="section level3">
<h3>Summary</h3>
<p>To reiterate, the difference between the Bayesian and frequentist approach to statistics boils down to how the data and parameters of a model are treated (fixed vs. random). I also think it’s easier to understand the difference in this way particularly for someone who already knows a bit about statistics. Hopefully this post will help to clarify things for those who end up in a similar situation as I was in.</p>
</div>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>I use the terms “a frequentist” or “a Bayesian” but I don’t see them as distinct in the sense that one should use whatever approach is more appropriate for the given situation.<a href="#fnref1">↩</a></p></li>
</ol>
</div>
